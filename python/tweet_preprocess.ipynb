{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "import re\n",
    "import emoji\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urlmarker import URL_REGEX # reference https://gist.github.com/gruber/8891611\n",
    "\n",
    "import inflect\n",
    "\n",
    "from spellchecker import SpellChecker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "[nltk_data] Downloading package stopwords to /home/george/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package punkt to /home/george/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package wordnet to /home/george/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "True"
     },
     "metadata": {},
     "execution_count": 113
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "      keyword location                                               text  \\\nid                                                                          \n1         NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n4         NaN      NaN             Forest fire near La Ronge Sask. Canada   \n5         NaN      NaN  All residents asked to 'shelter in place' are ...   \n6         NaN      NaN  13,000 people receive #wildfires evacuation or...   \n7         NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n...       ...      ...                                                ...   \n10869     NaN      NaN  Two giant cranes holding a bridge collapse int...   \n10870     NaN      NaN  @aria_ahrary @TheTawniest The out of control w...   \n10871     NaN      NaN  M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...   \n10872     NaN      NaN  Police investigating after an e-bike collided ...   \n10873     NaN      NaN  The Latest: More Homes Razed by Northern Calif...   \n\n       target  \nid             \n1           1  \n4           1  \n5           1  \n6           1  \n7           1  \n...       ...  \n10869       1  \n10870       1  \n10871       1  \n10872       1  \n10873       1  \n\n[7613 rows x 4 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>keyword</th>\n      <th>location</th>\n      <th>text</th>\n      <th>target</th>\n    </tr>\n    <tr>\n      <th>id</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Our Deeds are the Reason of this #earthquake M...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Forest fire near La Ronge Sask. Canada</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>All residents asked to 'shelter in place' are ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>13,000 people receive #wildfires evacuation or...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <td>10869</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Two giant cranes holding a bridge collapse int...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <td>10870</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>@aria_ahrary @TheTawniest The out of control w...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <td>10871</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <td>10872</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Police investigating after an e-bike collided ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <td>10873</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>The Latest: More Homes Razed by Northern Calif...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>7613 rows Ã— 4 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 114
    }
   ],
   "source": [
    "train_df = pd.read_excel('train.xlsx')\n",
    "train_df.set_index('id', inplace=True)\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "abbreviation_dict = {}\n",
    "with open(\"lingo.txt\") as f:\n",
    "    for line in f:\n",
    "       (key, val) = line.split('\\t')\n",
    "       abbreviation_dict[(key)] = val.replace('\\n', '')\n",
    "\n",
    "contraction_dict = {}\n",
    "with open(\"contractions.txt\") as f:\n",
    "    for line in f:\n",
    "       (key, val) = line.split(':')\n",
    "       contraction_dict[(key)] = val.replace('\\n', '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# URL related functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeURLs(tweet):\n",
    "    \"\"\"Replaces URLs in the tweet given with the string 'URL' \"\"\"\n",
    "    tweet = re.sub(URL_REGEX, 'URL', tweet)\n",
    "    return tweet\n",
    "\n",
    "def listURLs(tweet):\n",
    "    \"\"\"Returns a list of URLs contained in the given tweet\"\"\"\n",
    "    return re.findall(URL_REGEX, tweet)\n",
    "\n",
    "def extractTextFromURL(url):\n",
    "    \"\"\"Returns text from the given URL\"\"\"\n",
    "    res = requests.get(url)\n",
    "    html_page = res.content\n",
    "    soup = BeautifulSoup(html_page, 'html.parser')\n",
    "    text = soup.find_all(text=True)\n",
    "    output = ''\n",
    "    undesired = ['[document]', 'noscript',\n",
    "\t             'header', 'html',\n",
    "\t             'meta', 'head', \n",
    "                 'input', 'script',\n",
    "                 'style',]\n",
    "    for t in text:\n",
    "\t    if t.parent.name not in undesired:\n",
    "\t\t    output += '{} '.format(t)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove unwanted elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeNonAscii(tweet):\n",
    "    \"\"\"Remove non ascii characters\"\"\"\n",
    "    return tweet.encode('ascii', 'ignore').decode('ascii')\n",
    "\n",
    "def removePunctuation(tweet):\n",
    "    \"\"\"Remove punctuations - removes # as well\"\"\"\n",
    "    translator = str.maketrans(string.punctuation, ' '*len(string.punctuation))\n",
    "    return tweet.translate(translator)\n",
    "\n",
    "def removeNums(tweet):\n",
    "    \"\"\"Remove numbers\"\"\"\n",
    "    return ''.join([char for char in tweet if not char.isdigit()])\n",
    "\n",
    "def removeUsernames(tweet):\n",
    "    \"\"\"Remove usernames\"\"\"\n",
    "    return re.sub('@[^\\s]+', '', tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Format related functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def toLowerCase(tweet):\n",
    "    \"\"\"Separate camel case to space delimited and convert tweet to lower-case\"\"\"\n",
    "    tweet = re.sub(r'((?<=[a-z])[A-Z]|(?<!\\A)[A-Z](?=[a-z]))', r' \\1', tweet)\n",
    "    tweet = tweet.lower()\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Meaning related functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replaceEmojis(tweet):\n",
    "    \"\"\"Replace emojis in the text with their correspinding meaning\"\"\"\n",
    "    return emoji.demojize(tweet) #.replace(':', '')\n",
    "\n",
    "def replaceNums(tweet):\n",
    "    \"\"\"Replace integers with textual representation\"\"\"\n",
    "    infeng = inflect.engine()\n",
    "    processed_tweet = []\n",
    "    for word in nltk.word_tokenize(tweet):\n",
    "         processed_tweet.append(infeng.number_to_words(word) if word.isdigit() else word)\n",
    "    return ' '.join(processed_tweet)          \n",
    "\n",
    "def correctSpelling(tweet):\n",
    "    \"\"\"Corrects spelling in the given string\"\"\"\n",
    "    spell = SpellChecker()\n",
    "    spell.word_frequency.load_words(['url'])\n",
    "    words = nltk.word_tokenize(tweet)\n",
    "    # find those words that may be misspelled\n",
    "    misspelled = spell.unknown(words)\n",
    "    processed_tweet = []\n",
    "    for word in words:\n",
    "        # Replaced misspelled with the one most likely answer\n",
    "        processed_tweet.append(spell.correction(word) if word in misspelled else word)\n",
    "    return ' '.join(processed_tweet)\n",
    "\n",
    "def replaceAbbreviations(tweet):\n",
    "    \"\"\"Replaces abbreviation with the corresponding full text from dictionary\"\"\"\n",
    "    processed_tweet = []\n",
    "    for word in tweet.split():\n",
    "         processed_tweet.append(abbreviation_dict.get(word) if word in abbreviation_dict else word)\n",
    "    return ' '.join(processed_tweet)      \n",
    "\n",
    "def replaceContractions(tweet):\n",
    "    \"\"\"Replaces contractions with the corresponding full text from dictionary\"\"\"\n",
    "    processed_tweet = []\n",
    "    for word in tweet.split():\n",
    "         processed_tweet.append(contraction_dict.get(word) if word in contraction_dict else word)\n",
    "    return ' '.join(processed_tweet)  \n",
    "\n",
    "def removeStopWords(tweet):\n",
    "    \"\"\"Remove stopwords\"\"\"\n",
    "    return [word for word in nltk.word_tokenize(tweet) if word not in stopwords.words('english')]\n",
    "\n",
    "def lemmatization(tweet):\n",
    "    \"\"\"Lemmatization - reduces the word-forms to linguistically valid lemmas\"\"\"\n",
    "    return [WordNetLemmatizer().lemmatize(word) for word in tweet]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_tweet(tweet):\n",
    "    tweet = removeURLs(tweet)\n",
    "    tweet = replaceEmojis(tweet)\n",
    "    tweet = removeUsernames(tweet)\n",
    "    tweet = removeNonAscii(tweet)\n",
    "    tweet = toLowerCase(tweet)\n",
    "    tweet = replaceAbbreviations(tweet)\n",
    "    tweet = replaceContractions(tweet)\n",
    "    tweet = correctSpelling(tweet)\n",
    "    tweet = removePunctuation(tweet)\n",
    "    tweet = removeNums(tweet)\n",
    "    tweet = removeStopWords(tweet)\n",
    "    tweet = lemmatization(tweet)\n",
    "    return ' '.join(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'i dont even remember slsp happening i just remember being like wtf and then the lights turned off and everyone screamed for the encore'"
     },
     "metadata": {},
     "execution_count": 223
    }
   ],
   "source": [
    "train_df.loc[8491,'text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'even remember slip happening remember like fuck light turned everyone screamed encore'"
     },
     "metadata": {},
     "execution_count": 224
    }
   ],
   "source": [
    "preprocess_tweet(train_df.loc[8491,'text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'goole search image play mail drive calendar translate web logger web history setting sign advanced search goole offered take advertising program goole url copyright privacy term'"
     },
     "metadata": {},
     "execution_count": 225
    }
   ],
   "source": [
    "preprocess_tweet(extractTextFromURL('https://google.com/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'test twelve three t'"
     },
     "metadata": {},
     "execution_count": 226
    }
   ],
   "source": [
    "removeNums(replaceNums('test 12 3 t4'))"
   ]
  }
 ]
}