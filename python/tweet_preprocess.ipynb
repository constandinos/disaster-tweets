{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "import re\n",
    "import emoji\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urlmarker import URL_REGEX # reference https://gist.github.com/gruber/8891611\n",
    "\n",
    "import inflect\n",
    "\n",
    "from spellchecker import SpellChecker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('../dataset/train.csv')\n",
    "train_df.set_index('id', inplace=True)\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abbreviation_dict = {}\n",
    "with open(\"../dictionaries/abbreviations.txt\") as f:\n",
    "    for line in f:\n",
    "       (key, val) = line.split('\\t')\n",
    "       abbreviation_dict[(key)] = val.replace('\\n', '')\n",
    "\n",
    "contraction_dict = {}\n",
    "with open(\"../dictionaries/contractions.txt\") as f:\n",
    "    for line in f:\n",
    "       (key, val) = line.split(':')\n",
    "       contraction_dict[(key)] = val.replace('\\n', '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# URL related functions"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeURLs(tweet):\n",
    "    \"\"\"\n",
    "    Replaces URLs in the tweet given with the string 'URL'.\n",
    "    \n",
    "    Parameters:\n",
    "        tweet (string): tweet to be processed.\n",
    "\n",
    "    Returns:\n",
    "        string: given tweet with the URLs removed.\n",
    "    \"\"\"\n",
    "    tweet = re.sub(URL_REGEX, 'URL', tweet)\n",
    "    return tweet\n",
    "\n",
    "def listURLs(tweet):\n",
    "    \"\"\"\n",
    "    Returns a list of URLs contained in the given tweet.\n",
    "            \n",
    "    Parameters:\n",
    "        tweet (string): tweet to be processed.\n",
    "\n",
    "    Returns: \n",
    "        list: a list of URLs.\n",
    "    \"\"\"\n",
    "    return re.findall(URL_REGEX, tweet)\n",
    "\n",
    "def extractTextFromURLs(urls):\n",
    "    \"\"\"\n",
    "    Returns text from the given list of URL filtering out some HTML tags.\n",
    "        \n",
    "    Parameters:\n",
    "        url (list): list of URL to be processed.\n",
    "\n",
    "    Returns: \n",
    "        string: text extracted from the given URLs.\n",
    "    \"\"\"\n",
    "    extracted = ''\n",
    "    for url in urls:\n",
    "        res = requests.get(url)\n",
    "        html_page = res.content\n",
    "        soup = BeautifulSoup(html_page, 'html.parser')\n",
    "        text = soup.find_all(text=True)\n",
    "        \n",
    "        undesired = ['[document]', 'noscript',\n",
    "\t                'header', 'html',\n",
    "\t                'meta', 'head', \n",
    "                    'input', 'script',\n",
    "                    'style', 'title']\n",
    "        for t in text:\n",
    "\t        if t.parent.name not in undesired:\n",
    "\t\t        extracted += '{} '.format(t)\n",
    "\n",
    "    return extracted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove unwanted elements"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeNonAscii(tweet):\n",
    "    \"\"\"\n",
    "    Removes non ascii characters from given string.\n",
    "\n",
    "    Parameters:\n",
    "        tweet (string): tweet to be processed.\n",
    "    \n",
    "    Returns: \n",
    "        string: given tweet with non ascii characters removed.    \n",
    "    \"\"\"\n",
    "    return tweet.encode('ascii', 'ignore').decode('ascii')\n",
    "\n",
    "def removeNonPrintable(tweet):\n",
    "    \"\"\"\n",
    "    Removes non printable characters from given string.\n",
    "\n",
    "    Parameters:\n",
    "        tweet (string): tweet to be processed.\n",
    "    \n",
    "    Returns: \n",
    "        string: given tweet with non printable removed.    \n",
    "    \"\"\"\n",
    "    return ''.join(filter(lambda x: x in string.printable, tweet))\n",
    "\n",
    "def removePunctuation(tweet):\n",
    "    \"\"\"\n",
    "    Removes punctuations (removes # as well).\n",
    "\n",
    "    Parameters:\n",
    "        tweet (string): tweet to be processed.\n",
    "    \n",
    "    Returns:\n",
    "        string: given tweet with punctuations removed.\n",
    "    \"\"\"\n",
    "    translator = str.maketrans(string.punctuation, ' '*len(string.punctuation))\n",
    "    return tweet.translate(translator)\n",
    "\n",
    "def removeNums(tweet):\n",
    "    \"\"\"\n",
    "    Removes numeric values from the given string.\n",
    "    \n",
    "    Parameters:\n",
    "        tweet (string): tweet to be processed.\n",
    "    \n",
    "    Returns: \n",
    "        string: given tweet with numeric values removed.    \n",
    "    \"\"\"\n",
    "    return ''.join([char for char in tweet if not char.isdigit()])\n",
    "\n",
    "def removeUsernames(tweet):\n",
    "    \"\"\"\n",
    "    Removes usernames from given tweet.\n",
    "    \n",
    "    Parameters:\n",
    "        tweet (string): tweet to be processed.\n",
    "    \n",
    "    Returns: \n",
    "        string: given tweet with usernames removed.   \n",
    "    \"\"\"\n",
    "    return re.sub('@[^\\s]+', '', tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Format related functions"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def toLowerCase(tweet):\n",
    "    \"\"\"\n",
    "    Separate camelCase to space delimited and convert tweet to lower-case.\n",
    "    \n",
    "    Parameters:\n",
    "        tweet (string): tweet to be processed.\n",
    "    \n",
    "    Returns: \n",
    "        string: given tweet to lower case.\n",
    "    \"\"\"\n",
    "    tweet = re.sub(r'((?<=[a-z])[A-Z]|(?<!\\A)[A-Z](?=[a-z]))', r' \\1', tweet)\n",
    "    tweet = tweet.lower()\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Meaning related functions"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replaceEmojis(tweet):\n",
    "    \"\"\"\n",
    "    Replace emojis in the text with their correspinding meaning.\n",
    "    \n",
    "    Parameters:\n",
    "        tweet (string): tweet to be processed.\n",
    "    \n",
    "    Returns: \n",
    "        string: given tweet with emojis replaced.  \n",
    "    \"\"\"\n",
    "    return emoji.demojize(tweet) #.replace(':', '')\n",
    "\n",
    "def replaceNums(tweet):\n",
    "    \"\"\"\n",
    "    Replace numerical values with their textual representation.\n",
    "        \n",
    "    Parameters:\n",
    "        tweet (string): tweet to be processed.\n",
    "    \n",
    "    Returns: \n",
    "        string: given tweet with numerical values replaced.\n",
    "    \"\"\"\n",
    "    infeng = inflect.engine()\n",
    "    processed_tweet = []\n",
    "    for word in tweet.split():\n",
    "         processed_tweet.append(infeng.number_to_words(word) if word.isdigit() else word)\n",
    "    return ' '.join(processed_tweet)          \n",
    "\n",
    "def correctSpelling(tweet_list):\n",
    "    \"\"\"\n",
    "    Corrects spelling in the given string.\n",
    "    \n",
    "    Parameters:\n",
    "        tweet_list (list): list of string-words to be processed.\n",
    "    \n",
    "    Returns: \n",
    "        list: given tweet-list spelling-corrected.\n",
    "    \"\"\"\n",
    "    spell = SpellChecker()\n",
    "    spell.word_frequency.load_words(['url']) # add url to the dictionary\n",
    "    # find those words that may be misspelled\n",
    "    misspelled = spell.unknown(tweet_list)\n",
    "    processed_tweet = []\n",
    "    for word in tweet_list:\n",
    "        # Replaced misspelled with the one most likely answer\n",
    "        processed_tweet.append(spell.correction(word) if word in misspelled else word)\n",
    "    return processed_tweet\n",
    "\n",
    "def replaceAbbreviations(tweet_list, abbreviation_dict):\n",
    "    \"\"\"\n",
    "    Replaces abbreviation with the corresponding full text from dictionary.\n",
    "    \n",
    "    Parameters:\n",
    "        tweet_list (list): list of string-words to be processed.\n",
    "        abbreviation_dict (dictionary): dictionary of abbreviation.\n",
    "    \n",
    "    Returns: \n",
    "        list: given tweet-list with the abbreviations replaced.\n",
    "    \"\"\"\n",
    "    processed_list = []\n",
    "    for word in tweet_list:\n",
    "        if word in abbreviation_dict:\n",
    "            if len(abbreviation_dict.get(word).split()) > 1: # in case of multiple words\n",
    "                processed_list.extend(abbreviation_dict.get(word).split())\n",
    "            else:\n",
    "                processed_list.append(abbreviation_dict.get(word))\n",
    "        else:\n",
    "            processed_list.append(word)\n",
    "    return processed_list   \n",
    "\n",
    "def replaceContractions(tweet_list, contraction_dict):\n",
    "    \"\"\"\n",
    "    Replaces contractions with the corresponding full text from dictionary.\n",
    "        \n",
    "    Parameters:\n",
    "        tweet_list (list): list of string-words to be processed.\n",
    "        contraction_dict (dictionary): dictionary of contractions.\n",
    "    \n",
    "    Returns: \n",
    "        list: given tweet-list with the contractions replaced.\n",
    "    \"\"\"\n",
    "    processed_list = []\n",
    "    for word in tweet_list:\n",
    "        if word in contraction_dict:\n",
    "            if len(contraction_dict.get(word).split()) > 1: # in case of multiple words\n",
    "                processed_list.extend(contraction_dict.get(word).split())\n",
    "            else:\n",
    "                processed_list.append(contraction_dict.get(word))\n",
    "        else:\n",
    "            processed_list.append(word)\n",
    "    return processed_list \n",
    "\n",
    "def removeStopWords(tweet_list):\n",
    "    \"\"\"\n",
    "    Removes stop-words from the given tweet.\n",
    "        \n",
    "    Parameters:\n",
    "        tweet_list (list): list of string-words to be processed.\n",
    "    \n",
    "    Returns: \n",
    "        list: given tweet with stop-words removed.\n",
    "    \"\"\"\n",
    "    return [word for word in tweet_list if word not in stopwords.words('english')]\n",
    "\n",
    "def stemming(tweet_list):\n",
    "    \"\"\"\n",
    "    Stemming - reduces the word-forms by removing suffixes.\n",
    "\n",
    "    Parameters:\n",
    "        tweet_list (list): list of string-words to be processed.\n",
    "\n",
    "    Returns: \n",
    "        list: given tweet stemmed.\n",
    "    \"\"\"\n",
    "    return [PorterStemmer().stem(word) for word in tweet_list]\n",
    "\n",
    "def lemmatization(tweet_list):\n",
    "    \"\"\"\n",
    "    Lemmatization - reduces the word-forms to linguistically valid lemmas.\n",
    "\n",
    "    Parameters:\n",
    "        tweet_list (list): list of string-words to be processed.\n",
    "\n",
    "    Returns: \n",
    "        list: given tweet lemmatized.\n",
    "    \"\"\"\n",
    "    return [WordNetLemmatizer().lemmatize(word) for word in tweet_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_tweet(tweet, abbreviation_dict, contraction_dict):\n",
    "    tweet = replaceEmojis(tweet)\n",
    "    tweet = removeNonAscii(tweet)\n",
    "    tweet = removeURLs(tweet)\n",
    "    tweet = removeUsernames(tweet)\n",
    "    tweet = removeNonPrintable(tweet)\n",
    "    \n",
    "    tweet = toLowerCase(tweet)\n",
    "\n",
    "    tweet_list = tweet.split()\n",
    "    tweet_list = replaceAbbreviations(tweet_list, abbreviation_dict)\n",
    "    tweet_list = replaceContractions(tweet_list, contraction_dict)\n",
    "\n",
    "    tweet_list = (removeNums(' '.join(tweet_list))).split()\n",
    "    tweet_list = (removePunctuation(' '.join(tweet_list))).split()\n",
    "    \n",
    "    tweet_list = correctSpelling(tweet_list)\n",
    "    \n",
    "    tweet_list = removeStopWords(tweet_list)\n",
    "    tweet_list = lemmatization(tweet_list)\n",
    "    tweet_list = stemming(tweet_list)\n",
    "    return tweet_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.loc[8606,'text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_tweet(train_df.loc[8606,'text'], abbreviation_dict, contraction_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in train_df.loc[8607:8607].iterrows():\n",
    "    train_df.at[index, 'preprocessed text'] = ' '.join(preprocess_tweet(row['text'], abbreviation_dict, contraction_dict))\n",
    "    train_df.at[index, 'preprocessed extracted text from URLs'] = ' '.join([word for word in (preprocess_tweet(extractTextFromURLs(listURLs(row['text'])), abbreviation_dict, contraction_dict)) if len(word)>3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extractTextFromURLs(listURLs(train_df.loc[8607,'text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.loc[8607]"
   ]
  }
 ]
}