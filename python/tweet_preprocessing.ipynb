{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import string\n",
    "import re\n",
    "from emot.emo_unicode import UNICODE_EMO, EMOTICONS # reference https://github.com/NeelShah18/emot\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urlmarker import URL_REGEX # reference https://gist.github.com/gruber/8891611\n",
    "\n",
    "import inflect\n",
    "\n",
    "from spellchecker import SpellChecker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "[nltk_data] Downloading package stopwords to /home/george/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package punkt to /home/george/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package wordnet to /home/george/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "True"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "      keyword location                                               text  \\\nid                                                                          \n1         NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n4         NaN      NaN             Forest fire near La Ronge Sask. Canada   \n5         NaN      NaN  All residents asked to 'shelter in place' are ...   \n6         NaN      NaN  13,000 people receive #wildfires evacuation or...   \n7         NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n...       ...      ...                                                ...   \n10869     NaN      NaN  Two giant cranes holding a bridge collapse int...   \n10870     NaN      NaN  @aria_ahrary @TheTawniest The out of control w...   \n10871     NaN      NaN  M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...   \n10872     NaN      NaN  Police investigating after an e-bike collided ...   \n10873     NaN      NaN  The Latest: More Homes Razed by Northern Calif...   \n\n       target  \nid             \n1           1  \n4           1  \n5           1  \n6           1  \n7           1  \n...       ...  \n10869       1  \n10870       1  \n10871       1  \n10872       1  \n10873       1  \n\n[7613 rows x 4 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>keyword</th>\n      <th>location</th>\n      <th>text</th>\n      <th>target</th>\n    </tr>\n    <tr>\n      <th>id</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Our Deeds are the Reason of this #earthquake M...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Forest fire near La Ronge Sask. Canada</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>All residents asked to 'shelter in place' are ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>13,000 people receive #wildfires evacuation or...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <td>10869</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Two giant cranes holding a bridge collapse int...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <td>10870</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>@aria_ahrary @TheTawniest The out of control w...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <td>10871</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <td>10872</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Police investigating after an e-bike collided ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <td>10873</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>The Latest: More Homes Razed by Northern Calif...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>7613 rows Ã— 4 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 52
    }
   ],
   "source": [
    "train_df = pd.read_csv('../dataset/train.csv')\n",
    "train_df.set_index('id', inplace=True)\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "abbreviation_dict = {}\n",
    "with open(\"../dictionaries/abbreviations.txt\") as f:\n",
    "    for line in f:\n",
    "       (key, val) = line.split('\\t')\n",
    "       abbreviation_dict[(key)] = val.replace('\\n', '')\n",
    "\n",
    "contraction_dict = {}\n",
    "with open(\"../dictionaries/contractions.txt\") as f:\n",
    "    for line in f:\n",
    "       (key, val) = line.split(':')\n",
    "       contraction_dict[(key)] = val.replace('\\n', '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# URL related functions"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeURLs(tweet):\n",
    "    \"\"\"\n",
    "    Replaces URLs in the tweet given with the string 'URL'.\n",
    "    \n",
    "    Parameters:\n",
    "        tweet (string): tweet to be processed.\n",
    "\n",
    "    Returns:\n",
    "        string: given tweet with the URLs removed.\n",
    "    \"\"\"\n",
    "    tweet = re.sub(URL_REGEX, 'URL', tweet)\n",
    "    return tweet\n",
    "\n",
    "def listURLs(tweet):\n",
    "    \"\"\"\n",
    "    Returns a list of URLs contained in the given tweet.\n",
    "            \n",
    "    Parameters:\n",
    "        tweet (string): tweet to be processed.\n",
    "\n",
    "    Returns: \n",
    "        list: a list of URLs.\n",
    "    \"\"\"\n",
    "    return re.findall(URL_REGEX, tweet)\n",
    "\n",
    "def extractTextFromURLs(urls):\n",
    "    \"\"\"\n",
    "    Returns text from the given list of URL filtering out some HTML tags.\n",
    "        \n",
    "    Parameters:\n",
    "        url (list): list of URL to be processed.\n",
    "\n",
    "    Returns: \n",
    "        string: text extracted from the given URLs.\n",
    "    \"\"\"\n",
    "    extracted = ''\n",
    "    for url in urls:\n",
    "        try:\n",
    "            res = requests.get(url)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            continue\n",
    "                \n",
    "        html_page = res.content\n",
    "        soup = BeautifulSoup(html_page, 'html.parser')\n",
    "        text = soup.find_all(text=True)\n",
    "        \n",
    "        undesired = ['[document]', 'noscript',\n",
    "\t                'header', 'html',\n",
    "\t                'meta', 'head', \n",
    "                    'input', 'script',\n",
    "                    'style', 'title']\n",
    "        for t in text:\n",
    "\t        if t.parent.name not in undesired:\n",
    "\t\t        extracted += '{} '.format(t)\n",
    "\n",
    "    return extracted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove unwanted elements"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeNonAscii(tweet):\n",
    "    \"\"\"\n",
    "    Removes non ascii characters from given string.\n",
    "\n",
    "    Parameters:\n",
    "        tweet (string): tweet to be processed.\n",
    "    \n",
    "    Returns: \n",
    "        string: given tweet with non ascii characters removed.    \n",
    "    \"\"\"\n",
    "    return tweet.encode('ascii', 'ignore').decode('ascii')\n",
    "\n",
    "def removeNonPrintable(tweet):\n",
    "    \"\"\"\n",
    "    Removes non printable characters from given string.\n",
    "\n",
    "    Parameters:\n",
    "        tweet (string): tweet to be processed.\n",
    "    \n",
    "    Returns: \n",
    "        string: given tweet with non printable removed.    \n",
    "    \"\"\"\n",
    "    return ''.join(filter(lambda x: x in string.printable, tweet))\n",
    "\n",
    "def removePunctuation(tweet):\n",
    "    \"\"\"\n",
    "    Removes punctuations (removes # as well).\n",
    "\n",
    "    Parameters:\n",
    "        tweet (string): tweet to be processed.\n",
    "    \n",
    "    Returns:\n",
    "        string: given tweet with punctuations removed.\n",
    "    \"\"\"\n",
    "    translator = str.maketrans(string.punctuation, ' '*len(string.punctuation))\n",
    "    return tweet.translate(translator)\n",
    "\n",
    "def removeNums(tweet):\n",
    "    \"\"\"\n",
    "    Removes numeric values from the given string.\n",
    "    \n",
    "    Parameters:\n",
    "        tweet (string): tweet to be processed.\n",
    "    \n",
    "    Returns: \n",
    "        string: given tweet with numeric values removed.    \n",
    "    \"\"\"\n",
    "    return ''.join([char for char in tweet if not char.isdigit()])\n",
    "\n",
    "def removeUsernames(tweet):\n",
    "    \"\"\"\n",
    "    Removes usernames from given tweet.\n",
    "    \n",
    "    Parameters:\n",
    "        tweet (string): tweet to be processed.\n",
    "    \n",
    "    Returns: \n",
    "        string: given tweet with usernames removed.   \n",
    "    \"\"\"\n",
    "    return re.sub('@[^\\s]+', '', tweet)\n",
    "\n",
    "def removeRepeatedChars(tweet):\n",
    "    \"\"\"\n",
    "    Reduces repeated consecutive characters from given tweet to only two.\n",
    "    \n",
    "    Parameters:\n",
    "        tweet (string): tweet to be processed.\n",
    "    \n",
    "    Returns: \n",
    "        string: given tweet with repeated characters removed.   \n",
    "    \"\"\"\n",
    "    return re.sub(r'(.)\\1+', r'\\1\\1', tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Format related functions"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def toLowerCase(tweet):\n",
    "    \"\"\"\n",
    "    Separate camelCase to space delimited and convert tweet to lower-case.\n",
    "    \n",
    "    Parameters:\n",
    "        tweet (string): tweet to be processed.\n",
    "    \n",
    "    Returns: \n",
    "        string: given tweet to lower case.\n",
    "    \"\"\"\n",
    "    tweet = re.sub(r'((?<=[a-z])[A-Z]|(?<!\\A)[A-Z](?=[a-z]))', r' \\1', tweet)\n",
    "    tweet = tweet.lower()\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Meaning related functions"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replaceEmojis(tweet):\n",
    "    \"\"\"\n",
    "    Replace emojis in the text with their correspinding meaning.\n",
    "    \n",
    "    Parameters:\n",
    "        tweet (string): tweet to be processed.\n",
    "    \n",
    "    Returns: \n",
    "        string: given tweet with emojis replaced.  \n",
    "    \"\"\"\n",
    "    for emot in UNICODE_EMO:\n",
    "        tweet = tweet.replace(emot, \"_\".join(UNICODE_EMO[emot].replace(\",\",\"\").replace(\":\",\"\").split()))\n",
    "    return tweet\n",
    "\n",
    "def replaceEmoticons(tweet):\n",
    "    \"\"\"\n",
    "    Replace emoticons in the text with their correspinding meaning.\n",
    "    \n",
    "    Parameters:\n",
    "        tweet (string): tweet to be processed.\n",
    "    \n",
    "    Returns: \n",
    "        string: given tweet with emoticons replaced.  \n",
    "    \"\"\"\n",
    "    for emot in EMOTICONS:\n",
    "        tweet = re.sub(u'('+emot+')', \"_\".join(EMOTICONS[emot].replace(\",\",\"\").split()), tweet)\n",
    "    return tweet\n",
    "\n",
    "def replaceNums(tweet):\n",
    "    \"\"\"\n",
    "    Replace numerical values with their textual representation.\n",
    "        \n",
    "    Parameters:\n",
    "        tweet (string): tweet to be processed.\n",
    "    \n",
    "    Returns: \n",
    "        string: given tweet with numerical values replaced.\n",
    "    \"\"\"\n",
    "    infeng = inflect.engine()\n",
    "    processed_tweet = []\n",
    "    for word in tweet.split():\n",
    "         processed_tweet.append(infeng.number_to_words(word) if word.isdigit() else word)\n",
    "    return ' '.join(processed_tweet)          \n",
    "\n",
    "def correctSpelling(tweet_list):\n",
    "    \"\"\"\n",
    "    Corrects spelling in the given string.\n",
    "    \n",
    "    Parameters:\n",
    "        tweet_list (list): list of string-words to be processed.\n",
    "    \n",
    "    Returns: \n",
    "        list: given tweet-list spelling-corrected.\n",
    "    \"\"\"\n",
    "    spell = SpellChecker()\n",
    "    spell.word_frequency.load_words(['url']) # add url to the dictionary\n",
    "    # find those words that may be misspelled\n",
    "    misspelled = spell.unknown(tweet_list)\n",
    "    processed_tweet = []\n",
    "    for word in tweet_list:\n",
    "        # Replaced misspelled with the one most likely answer\n",
    "        processed_tweet.append(spell.correction(word) if word in misspelled else word)\n",
    "    return processed_tweet\n",
    "\n",
    "def replaceAbbreviations(tweet_list, abbreviation_dict):\n",
    "    \"\"\"\n",
    "    Replaces abbreviation with the corresponding full text from dictionary.\n",
    "    \n",
    "    Parameters:\n",
    "        tweet_list (list): list of string-words to be processed.\n",
    "        abbreviation_dict (dictionary): dictionary of abbreviation.\n",
    "    \n",
    "    Returns: \n",
    "        list: given tweet-list with the abbreviations replaced.\n",
    "    \"\"\"\n",
    "    processed_list = []\n",
    "    for word in tweet_list:\n",
    "        if word in abbreviation_dict:\n",
    "            if len(abbreviation_dict.get(word).split()) > 1: # in case of multiple words\n",
    "                processed_list.extend(abbreviation_dict.get(word).split())\n",
    "            else:\n",
    "                processed_list.append(abbreviation_dict.get(word))\n",
    "        else:\n",
    "            processed_list.append(word)\n",
    "    return processed_list   \n",
    "\n",
    "def replaceContractions(tweet_list, contraction_dict):\n",
    "    \"\"\"\n",
    "    Replaces contractions with the corresponding full text from dictionary.\n",
    "        \n",
    "    Parameters:\n",
    "        tweet_list (list): list of string-words to be processed.\n",
    "        contraction_dict (dictionary): dictionary of contractions.\n",
    "    \n",
    "    Returns: \n",
    "        list: given tweet-list with the contractions replaced.\n",
    "    \"\"\"\n",
    "    processed_list = []\n",
    "    for word in tweet_list:\n",
    "        if word in contraction_dict:\n",
    "            if len(contraction_dict.get(word).split()) > 1: # in case of multiple words\n",
    "                processed_list.extend(contraction_dict.get(word).split())\n",
    "            else:\n",
    "                processed_list.append(contraction_dict.get(word))\n",
    "        else:\n",
    "            processed_list.append(word)\n",
    "    return processed_list \n",
    "\n",
    "def removeStopWords(tweet_list):\n",
    "    \"\"\"\n",
    "    Removes stop-words from the given tweet.\n",
    "        \n",
    "    Parameters:\n",
    "        tweet_list (list): list of string-words to be processed.\n",
    "    \n",
    "    Returns: \n",
    "        list: given tweet with stop-words removed.\n",
    "    \"\"\"\n",
    "    return [word for word in tweet_list if word not in stopwords.words('english')]\n",
    "\n",
    "def stemming(tweet_list):\n",
    "    \"\"\"\n",
    "    Stemming - reduces the word-forms by removing suffixes.\n",
    "\n",
    "    Parameters:\n",
    "        tweet_list (list): list of string-words to be processed.\n",
    "\n",
    "    Returns: \n",
    "        list: given tweet stemmed.\n",
    "    \"\"\"\n",
    "    return [PorterStemmer().stem(word) for word in tweet_list]\n",
    "\n",
    "def lemmatization(tweet_list):\n",
    "    \"\"\"\n",
    "    Lemmatization - reduces the word-forms to linguistically valid lemmas.\n",
    "\n",
    "    Parameters:\n",
    "        tweet_list (list): list of string-words to be processed.\n",
    "\n",
    "    Returns: \n",
    "        list: given tweet lemmatized.\n",
    "    \"\"\"\n",
    "    return [WordNetLemmatizer().lemmatize(word) for word in tweet_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_tweet(tweet, abbreviation_dict, contraction_dict):\n",
    "    tweet = removeURLs(tweet)\n",
    "    tweet = removeUsernames(tweet)\n",
    "    tweet = replaceEmojis(tweet)\n",
    "    tweet = replaceEmoticons(tweet)\n",
    "    tweet = removeNonAscii(tweet)\n",
    "    tweet = removeNonPrintable(tweet)\n",
    "    tweet = removeRepeatedChars(tweet)\n",
    "    \n",
    "    tweet = toLowerCase(tweet)\n",
    "\n",
    "    tweet_list = tweet.split()\n",
    "    tweet_list = replaceAbbreviations(tweet_list, abbreviation_dict)\n",
    "    tweet_list = replaceContractions(tweet_list, contraction_dict)\n",
    "\n",
    "    tweet_list = (removeNums(' '.join(tweet_list))).split()\n",
    "    tweet_list = (removePunctuation(' '.join(tweet_list))).split()\n",
    "    \n",
    "    tweet_list = correctSpelling(tweet_list)\n",
    "    \n",
    "    tweet_list = removeStopWords(tweet_list)\n",
    "    tweet_list = lemmatization(tweet_list)\n",
    "    #tweet_list = stemming(tweet_list)\n",
    "    return tweet_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "record #1 processing finished\nrecord #4 processing finished\nrecord #5 processing finished\nrecord #6 processing finished\nrecord #7 processing finished\nrecord #8 processing finished\nrecord #10 processing finished\nrecord #13 processing finished\nrecord #14 processing finished\nrecord #15 processing finished\nrecord #16 processing finished\nrecord #17 processing finished\nrecord #18 processing finished\nrecord #19 processing finished\nrecord #20 processing finished\nrecord #23 processing finished\nrecord #24 processing finished\nrecord #25 processing finished\nrecord #26 processing finished\nrecord #28 processing finished\nrecord #31 processing finished\nrecord #32 processing finished\nrecord #33 processing finished\nrecord #34 processing finished\nrecord #36 processing finished\nrecord #37 processing finished\nrecord #38 processing finished\nrecord #39 processing finished\nrecord #40 processing finished\nrecord #41 processing finished\nrecord #44 processing finished\nrecord #48 processing finished\nrecord #49 processing finished\nrecord #50 processing finished\nrecord #52 processing finished\nrecord #53 processing finished\nrecord #54 processing finished\nrecord #55 processing finished\nrecord #56 processing finished\nrecord #57 processing finished\n"
    }
   ],
   "source": [
    "for index, row in train_df.iterrows():\n",
    "    train_df.at[index, 'processed_text'] = ' '.join(preprocess_tweet(row['text'], abbreviation_dict, contraction_dict))\n",
    " \n",
    "    urltext = extractTextFromURLs(listURLs(row['text']))\n",
    "    if(not urltext):\n",
    "        train_df.at[index, 'processed_URLs'] = 'NaN'\n",
    "    elif any(word in urltext.lower() for word in ['not found', 'unavailable', 'error', '404', 'not available', 'isâ€™t available', 'access is denied', 'page doesnâ€™t exist']):\n",
    "        train_df.at[index, 'processed_URLs'] = 'Page not found'\n",
    "    else:\n",
    "        train_df.at[index, 'processed_URLs'] = \"Page found\"\n",
    "    print(\"record #{} processing finished\".format(index))\n",
    "\n",
    "train_df.to_csv('../dataset/train_processed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "   keyword                       location  \\\nid                                          \n1      NaN                            NaN   \n4      NaN                            NaN   \n5      NaN                            NaN   \n6      NaN                            NaN   \n7      NaN                            NaN   \n8      NaN                            NaN   \n10     NaN                            NaN   \n13     NaN                            NaN   \n14     NaN                            NaN   \n15     NaN                            NaN   \n16     NaN                            NaN   \n17     NaN                            NaN   \n18     NaN                            NaN   \n19     NaN                            NaN   \n20     NaN                            NaN   \n23     NaN                            NaN   \n24     NaN                            NaN   \n25     NaN                            NaN   \n26     NaN                            NaN   \n28     NaN                            NaN   \n31     NaN                            NaN   \n32     NaN                            NaN   \n33     NaN                            NaN   \n34     NaN                            NaN   \n36     NaN                            NaN   \n37     NaN                            NaN   \n38     NaN                            NaN   \n39     NaN                            NaN   \n40     NaN                            NaN   \n41     NaN                            NaN   \n44     NaN                            NaN   \n48  ablaze                     Birmingham   \n49  ablaze  Est. September 2012 - Bristol   \n50  ablaze                         AFRICA   \n52  ablaze               Philadelphia, PA   \n53  ablaze                     London, UK   \n54  ablaze                       Pretoria   \n55  ablaze                   World Wide!!   \n56  ablaze                            NaN   \n57  ablaze                 Paranaque City   \n\n                                                 text  target  \\\nid                                                              \n1   Our Deeds are the Reason of this #earthquake M...       1   \n4              Forest fire near La Ronge Sask. Canada       1   \n5   All residents asked to 'shelter in place' are ...       1   \n6   13,000 people receive #wildfires evacuation or...       1   \n7   Just got sent this photo from Ruby #Alaska as ...       1   \n8   #RockyFire Update => California Hwy. 20 closed...       1   \n10  #flood #disaster Heavy rain causes flash flood...       1   \n13  I'm on top of the hill and I can see a fire in...       1   \n14  There's an emergency evacuation happening now ...       1   \n15  I'm afraid that the tornado is coming to our a...       1   \n16        Three people died from the heat wave so far       1   \n17  Haha South Tampa is getting flooded hah- WAIT ...       1   \n18  #raining #flooding #Florida #TampaBay #Tampa 1...       1   \n19            #Flood in Bago Myanmar #We arrived Bago       1   \n20  Damage to school bus on 80 in multi car crash ...       1   \n23                                     What's up man?       0   \n24                                      I love fruits       0   \n25                                   Summer is lovely       0   \n26                                  My car is so fast       0   \n28                       What a goooooooaaaaaal!!!!!!       0   \n31                             this is ridiculous....       0   \n32                                  London is cool ;)       0   \n33                                        Love skiing       0   \n34                              What a wonderful day!       0   \n36                                           LOOOOOOL       0   \n37                     No way...I can't eat that shit       0   \n38                              Was in NYC last week!       0   \n39                                 Love my girlfriend       0   \n40                                          Cooool :)       0   \n41                                 Do you like pasta?       0   \n44                                           The end!       0   \n48  @bbcmtd Wholesale Markets ablaze http://t.co/l...       1   \n49  We always try to bring the heavy. #metal #RT h...       0   \n50  #AFRICANBAZE: Breaking news:Nigeria flag set a...       1   \n52                 Crying out for more! Set me ablaze       0   \n53  On plus side LOOK AT THE SKY LAST NIGHT IT WAS...       0   \n54  @PhDSquares #mufc they've built so much hype a...       0   \n55  INEC Office in Abia Set Ablaze - http://t.co/3...       1   \n56  Barbados #Bridgetown JAMAICA Â‰Ã›Ã’ Two cars set ...       1   \n57                             Ablaze for you Lord :D       0   \n\n                                       processed_text  processed_URLs  \nid                                                                     \n1   deed reason earthquake may allah forgive Unite...             NaN  \n4                forest fire near la range ask canada             NaN  \n5   resident asked shelter place notified officer ...             NaN  \n6   people receive wildfire evacuation order calif...             NaN  \n7   got sent photo ruby alaska smoke wildfire pour...             NaN  \n8   rocky fire update california closed direction ...             NaN  \n10  flood disaster heavy rain cause flash flooding...             NaN  \n13                           I top hill see fire wood             NaN  \n14  emergency evacuation happening building across...             NaN  \n15                       I afraid tornado coming area             NaN  \n16        three people died heat wave significant far             NaN  \n17  aha south tampa getting flooded hah wait secon...             NaN  \n18  raining flooding florida tampa bay tampa day I...             NaN  \n19                      flood ago myanmar arrived ago             NaN  \n20         damage school bus multi car crash breaking             NaN  \n23                                                man             NaN  \n24                                         love fruit             NaN  \n25                                      summer lovely             NaN  \n26                               car significant fast             NaN  \n28                                               goal             NaN  \n31                                         ridiculous             NaN  \n32                             london cool wink smirk             NaN  \n33                                        love skiing             NaN  \n34                                      wonderful day             NaN  \n36                         laughing outrageously loud             NaN  \n37                                way cannon eat shit             NaN  \n38                            New York City last week             NaN  \n39                                    love girlfriend             NaN  \n40                             cool happy face smiley             NaN  \n41                                         like pasta             NaN  \n44                                                end             NaN  \n48  wholesale market ablaze hyper text transfer pr...  Page not found  \n49  always try bring heavy metal rt hyper text tra...  Page not found  \n50  africanbaze breaking news nigeria flag set abl...  Page not found  \n52                                     cry set ablaze             NaN  \n53  plus side look sky last night ablaze hyper tex...  Page not found  \n54  mufc They built significant much hype around n...             NaN  \n55  inc office asia set ablaze hyper text transfer...      Page found  \n56  barbados bridgetown jamaica two car set ablaze...  Page not found  \n57          ablaze lord laughing big grin laugh glass             NaN  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>keyword</th>\n      <th>location</th>\n      <th>text</th>\n      <th>target</th>\n      <th>processed_text</th>\n      <th>processed_URLs</th>\n    </tr>\n    <tr>\n      <th>id</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Our Deeds are the Reason of this #earthquake M...</td>\n      <td>1</td>\n      <td>deed reason earthquake may allah forgive Unite...</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Forest fire near La Ronge Sask. Canada</td>\n      <td>1</td>\n      <td>forest fire near la range ask canada</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>All residents asked to 'shelter in place' are ...</td>\n      <td>1</td>\n      <td>resident asked shelter place notified officer ...</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>13,000 people receive #wildfires evacuation or...</td>\n      <td>1</td>\n      <td>people receive wildfire evacuation order calif...</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n      <td>1</td>\n      <td>got sent photo ruby alaska smoke wildfire pour...</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>#RockyFire Update =&gt; California Hwy. 20 closed...</td>\n      <td>1</td>\n      <td>rocky fire update california closed direction ...</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>#flood #disaster Heavy rain causes flash flood...</td>\n      <td>1</td>\n      <td>flood disaster heavy rain cause flash flooding...</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <td>13</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>I'm on top of the hill and I can see a fire in...</td>\n      <td>1</td>\n      <td>I top hill see fire wood</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <td>14</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>There's an emergency evacuation happening now ...</td>\n      <td>1</td>\n      <td>emergency evacuation happening building across...</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <td>15</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>I'm afraid that the tornado is coming to our a...</td>\n      <td>1</td>\n      <td>I afraid tornado coming area</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <td>16</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Three people died from the heat wave so far</td>\n      <td>1</td>\n      <td>three people died heat wave significant far</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <td>17</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Haha South Tampa is getting flooded hah- WAIT ...</td>\n      <td>1</td>\n      <td>aha south tampa getting flooded hah wait secon...</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <td>18</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>#raining #flooding #Florida #TampaBay #Tampa 1...</td>\n      <td>1</td>\n      <td>raining flooding florida tampa bay tampa day I...</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <td>19</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>#Flood in Bago Myanmar #We arrived Bago</td>\n      <td>1</td>\n      <td>flood ago myanmar arrived ago</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Damage to school bus on 80 in multi car crash ...</td>\n      <td>1</td>\n      <td>damage school bus multi car crash breaking</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <td>23</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>What's up man?</td>\n      <td>0</td>\n      <td>man</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <td>24</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>I love fruits</td>\n      <td>0</td>\n      <td>love fruit</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <td>25</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Summer is lovely</td>\n      <td>0</td>\n      <td>summer lovely</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <td>26</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>My car is so fast</td>\n      <td>0</td>\n      <td>car significant fast</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <td>28</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>What a goooooooaaaaaal!!!!!!</td>\n      <td>0</td>\n      <td>goal</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <td>31</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>this is ridiculous....</td>\n      <td>0</td>\n      <td>ridiculous</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <td>32</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>London is cool ;)</td>\n      <td>0</td>\n      <td>london cool wink smirk</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <td>33</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Love skiing</td>\n      <td>0</td>\n      <td>love skiing</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <td>34</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>What a wonderful day!</td>\n      <td>0</td>\n      <td>wonderful day</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <td>36</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>LOOOOOOL</td>\n      <td>0</td>\n      <td>laughing outrageously loud</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <td>37</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>No way...I can't eat that shit</td>\n      <td>0</td>\n      <td>way cannon eat shit</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <td>38</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Was in NYC last week!</td>\n      <td>0</td>\n      <td>New York City last week</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <td>39</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Love my girlfriend</td>\n      <td>0</td>\n      <td>love girlfriend</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Cooool :)</td>\n      <td>0</td>\n      <td>cool happy face smiley</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <td>41</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Do you like pasta?</td>\n      <td>0</td>\n      <td>like pasta</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <td>44</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>The end!</td>\n      <td>0</td>\n      <td>end</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <td>48</td>\n      <td>ablaze</td>\n      <td>Birmingham</td>\n      <td>@bbcmtd Wholesale Markets ablaze http://t.co/l...</td>\n      <td>1</td>\n      <td>wholesale market ablaze hyper text transfer pr...</td>\n      <td>Page not found</td>\n    </tr>\n    <tr>\n      <td>49</td>\n      <td>ablaze</td>\n      <td>Est. September 2012 - Bristol</td>\n      <td>We always try to bring the heavy. #metal #RT h...</td>\n      <td>0</td>\n      <td>always try bring heavy metal rt hyper text tra...</td>\n      <td>Page not found</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>ablaze</td>\n      <td>AFRICA</td>\n      <td>#AFRICANBAZE: Breaking news:Nigeria flag set a...</td>\n      <td>1</td>\n      <td>africanbaze breaking news nigeria flag set abl...</td>\n      <td>Page not found</td>\n    </tr>\n    <tr>\n      <td>52</td>\n      <td>ablaze</td>\n      <td>Philadelphia, PA</td>\n      <td>Crying out for more! Set me ablaze</td>\n      <td>0</td>\n      <td>cry set ablaze</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <td>53</td>\n      <td>ablaze</td>\n      <td>London, UK</td>\n      <td>On plus side LOOK AT THE SKY LAST NIGHT IT WAS...</td>\n      <td>0</td>\n      <td>plus side look sky last night ablaze hyper tex...</td>\n      <td>Page not found</td>\n    </tr>\n    <tr>\n      <td>54</td>\n      <td>ablaze</td>\n      <td>Pretoria</td>\n      <td>@PhDSquares #mufc they've built so much hype a...</td>\n      <td>0</td>\n      <td>mufc They built significant much hype around n...</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <td>55</td>\n      <td>ablaze</td>\n      <td>World Wide!!</td>\n      <td>INEC Office in Abia Set Ablaze - http://t.co/3...</td>\n      <td>1</td>\n      <td>inc office asia set ablaze hyper text transfer...</td>\n      <td>Page found</td>\n    </tr>\n    <tr>\n      <td>56</td>\n      <td>ablaze</td>\n      <td>NaN</td>\n      <td>Barbados #Bridgetown JAMAICA Â‰Ã›Ã’ Two cars set ...</td>\n      <td>1</td>\n      <td>barbados bridgetown jamaica two car set ablaze...</td>\n      <td>Page not found</td>\n    </tr>\n    <tr>\n      <td>57</td>\n      <td>ablaze</td>\n      <td>Paranaque City</td>\n      <td>Ablaze for you Lord :D</td>\n      <td>0</td>\n      <td>ablaze lord laughing big grin laugh glass</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 94
    }
   ],
   "source": [
    "train_df[:40]"
   ]
  }
 ]
}